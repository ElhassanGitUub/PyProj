{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaNDzwzIvFwNwgHAqES9WE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElhassanGitUub/PyProj/blob/main/Removing_Duplicates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **Lab 7: Removing Duplicates**\n"
      ],
      "metadata": {
        "id": "3U9IrsP1iUzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "In this lab, you will focus on data wrangling, an important step in preparing data for analysis. Data wrangling involves cleaning and organizing data to make it suitable for analysis. One key task in this process is removing duplicate entries, which are repeated entries that can distort analysis and lead to inaccurate conclusions.\n",
        "\n",
        "**Objectives**\n",
        "In this lab you will perform the following:\n",
        "\n",
        "1-Identify duplicate rows in the dataset.\n",
        "2-Use suitable techniques to remove duplicate rows and verify the removal.\n",
        "3-Summarize how to handle missing values appropriately.\n",
        "Use ConvertedCompYearly to normalize compensation data."
      ],
      "metadata": {
        "id": "kJvG95M1ixiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the Required Libraries\n"
      ],
      "metadata": {
        "id": "ZuYikgAOww5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Required Libraries\n"
      ],
      "metadata": {
        "id": "WsFSErvaw1bn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fglJgYpaiPTK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Load the Dataset into a DataFrame\n"
      ],
      "metadata": {
        "id": "UmFp8AkQxFkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL of the dataset\n",
        "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
        "\n",
        "# Load the dataset into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows to ensure it loaded correctly\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZPi0b3alwnRA",
        "outputId": "6851c387-4b8b-459e-93a0-5691cb7dc19c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ResponseId                      MainBranch                 Age  \\\n",
            "0           1  I am a developer by profession  Under 18 years old   \n",
            "1           2  I am a developer by profession     35-44 years old   \n",
            "2           3  I am a developer by profession     45-54 years old   \n",
            "3           4           I am learning to code     18-24 years old   \n",
            "4           5  I am a developer by profession     18-24 years old   \n",
            "\n",
            "            Employment RemoteWork   Check  \\\n",
            "0  Employed, full-time     Remote  Apples   \n",
            "1  Employed, full-time     Remote  Apples   \n",
            "2  Employed, full-time     Remote  Apples   \n",
            "3   Student, full-time        NaN  Apples   \n",
            "4   Student, full-time        NaN  Apples   \n",
            "\n",
            "                                    CodingActivities  \\\n",
            "0                                              Hobby   \n",
            "1  Hobby;Contribute to open-source projects;Other...   \n",
            "2  Hobby;Contribute to open-source projects;Other...   \n",
            "3                                                NaN   \n",
            "4                                                NaN   \n",
            "\n",
            "                                             EdLevel  \\\n",
            "0                          Primary/elementary school   \n",
            "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
            "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
            "3  Some college/university study without earning ...   \n",
            "4  Secondary school (e.g. American high school, G...   \n",
            "\n",
            "                                           LearnCode  \\\n",
            "0                             Books / Physical media   \n",
            "1  Books / Physical media;Colleague;On the job tr...   \n",
            "2  Books / Physical media;Colleague;On the job tr...   \n",
            "3  Other online resources (e.g., videos, blogs, f...   \n",
            "4  Other online resources (e.g., videos, blogs, f...   \n",
            "\n",
            "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
            "0                                                NaN  ...            NaN   \n",
            "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
            "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
            "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
            "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
            "\n",
            "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
            "0            NaN            NaN            NaN             NaN   \n",
            "1            0.0            0.0            0.0             0.0   \n",
            "2            NaN            NaN            NaN             NaN   \n",
            "3            NaN            NaN            NaN             NaN   \n",
            "4            NaN            NaN            NaN             NaN   \n",
            "\n",
            "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
            "0             NaN                    NaN        NaN                 NaN    NaN  \n",
            "1             0.0                    NaN        NaN                 NaN    NaN  \n",
            "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
            "3             NaN               Too long       Easy                 NaN    NaN  \n",
            "4             NaN              Too short       Easy                 NaN    NaN  \n",
            "\n",
            "[5 rows x 114 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Identifying Duplicate Rows**\n",
        "\n",
        "Task 1: Identify Duplicate Rows\n",
        "\n",
        "Count the number of duplicate rows in the dataset.\n",
        "Display the first few duplicate rows to understand their structure."
      ],
      "metadata": {
        "id": "oPPFZjKHiTPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "\n",
        "# Extract duplicate rows (keeping all occurrences)\n",
        "duplicates = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Display the first few duplicate rows\n",
        "print(\"First few duplicate rows:\")\n",
        "print(duplicates.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65d5gnnayKKr",
        "outputId": "472c9f2f-8c05-4fe8-aff9-917e469c672d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 0\n",
            "First few duplicate rows:\n",
            "Empty DataFrame\n",
            "Columns: [ResponseId, MainBranch, Age, Employment, RemoteWork, Check, CodingActivities, EdLevel, LearnCode, LearnCodeOnline, TechDoc, YearsCode, YearsCodePro, DevType, OrgSize, PurchaseInfluence, BuyNewTool, BuildvsBuy, TechEndorse, Country, Currency, CompTotal, LanguageHaveWorkedWith, LanguageWantToWorkWith, LanguageAdmired, DatabaseHaveWorkedWith, DatabaseWantToWorkWith, DatabaseAdmired, PlatformHaveWorkedWith, PlatformWantToWorkWith, PlatformAdmired, WebframeHaveWorkedWith, WebframeWantToWorkWith, WebframeAdmired, EmbeddedHaveWorkedWith, EmbeddedWantToWorkWith, EmbeddedAdmired, MiscTechHaveWorkedWith, MiscTechWantToWorkWith, MiscTechAdmired, ToolsTechHaveWorkedWith, ToolsTechWantToWorkWith, ToolsTechAdmired, NEWCollabToolsHaveWorkedWith, NEWCollabToolsWantToWorkWith, NEWCollabToolsAdmired, OpSysPersonal use, OpSysProfessional use, OfficeStackAsyncHaveWorkedWith, OfficeStackAsyncWantToWorkWith, OfficeStackAsyncAdmired, OfficeStackSyncHaveWorkedWith, OfficeStackSyncWantToWorkWith, OfficeStackSyncAdmired, AISearchDevHaveWorkedWith, AISearchDevWantToWorkWith, AISearchDevAdmired, NEWSOSites, SOVisitFreq, SOAccount, SOPartFreq, SOHow, SOComm, AISelect, AISent, AIBen, AIAcc, AIComplex, AIToolCurrently Using, AIToolInterested in Using, AIToolNot interested in Using, AINextMuch more integrated, AINextNo change, AINextMore integrated, AINextLess integrated, AINextMuch less integrated, AIThreat, AIEthics, AIChallenges, TBranch, ICorPM, WorkExp, Knowledge_1, Knowledge_2, Knowledge_3, Knowledge_4, Knowledge_5, Knowledge_6, Knowledge_7, Knowledge_8, Knowledge_9, Frequency_1, Frequency_2, Frequency_3, TimeSearching, TimeAnswering, Frustration, ProfessionalTech, ProfessionalCloud, ProfessionalQuestion, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 114 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Removing Duplicate Rows**\n",
        "Task 2: Remove Duplicates\n",
        "\n",
        "Remove duplicate rows from the dataset using the drop_duplicates() function.\n",
        "Verify the removal by counting the number of duplicate rows after removal ."
      ],
      "metadata": {
        "id": "Bwcs6akbxqJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count duplicate rows before removal\n",
        "initial_duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows before removal: {initial_duplicates}\")\n",
        "\n",
        "# Remove duplicate rows (keeping the first occurrence)\n",
        "df_cleaned = df.drop_duplicates()\n",
        "\n",
        "# Count duplicate rows after removal\n",
        "remaining_duplicates = df_cleaned.duplicated().sum()\n",
        "print(f\"Number of duplicate rows after removal: {remaining_duplicates}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A75zHW0zNpB",
        "outputId": "5061e63b-730a-4042-d46d-6cb28c985166"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows before removal: 0\n",
            "Number of duplicate rows after removal: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Outcome**\n",
        "Number of duplicate rows before removal\n",
        "Number of duplicate rows after removal (should be 0)\n",
        "Comparison of dataset shape before and after cleaning"
      ],
      "metadata": {
        "id": "uEJ7Ookr2XMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Handling Missing Values**\n",
        "\n",
        "Task 3: Identify and Handle Missing Values\n",
        "\n",
        "Identify missing values for all columns in the dataset.\n",
        "Choose a column with significant missing values (e.g., EdLevel) and impute with the most frequent value."
      ],
      "metadata": {
        "id": "Oaq6FIm6zdfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing Values Per Column:\\n\", missing_values)\n",
        "\n",
        "# Select a column with significant missing values (e.g., 'EdLevel')\n",
        "column_to_impute = \"EdLevel\"\n",
        "\n",
        "# Check the most frequent value in the selected column\n",
        "most_frequent_value = df[column_to_impute].mode()[0]\n",
        "print(f\"Most frequent value in '{column_to_impute}': {most_frequent_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NKuAtVnGzgTB",
        "outputId": "81677ed6-8661-4fe1-d6e6-a26b2a5690ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values Per Column:\n",
            " ResponseId                 0\n",
            "MainBranch                 0\n",
            "Age                        0\n",
            "Employment                 0\n",
            "RemoteWork             10631\n",
            "                       ...  \n",
            "JobSatPoints_11        35992\n",
            "SurveyLength            9255\n",
            "SurveyEase              9199\n",
            "ConvertedCompYearly    42002\n",
            "JobSat                 36311\n",
            "Length: 114, dtype: int64\n",
            "Most frequent value in 'EdLevel': Bachelor’s degree (B.A., B.S., B.Eng., etc.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What This Code Does**\n",
        "Identifies missing values for each column.\n",
        "Selects a column with significant missing values (EdLevel).\n",
        "Finds the most frequent value in that column.\n",
        "Fills missing values with the most frequent value.\n",
        "Verifies that missing values are handled."
      ],
      "metadata": {
        "id": "h1hCNaG913H-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2bH9lp0Y0RvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Normalizing Compensation Data**\n",
        "\n",
        "Task 4: Normalize Compensation Data Using ConvertedCompYearly\n",
        "\n",
        "Use the ConvertedCompYearly column for compensation analysis as the normalized annual compensation is already provided.\n",
        "Check for missing values in ConvertedCompYearly and handle them if necessary.\n"
      ],
      "metadata": {
        "id": "hVqdULbm0Qg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in ConvertedCompYearly\n",
        "missing_comp = df[\"ConvertedCompYearly\"].isnull().sum()\n",
        "print(f\"Missing values in ConvertedCompYearly: {missing_comp}\")\n",
        "\n",
        "# Handle missing values - Fill with median compensation\n",
        "median_comp = df[\"ConvertedCompYearly\"].median()\n",
        "df[\"ConvertedCompYearly\"].fillna(median_comp, inplace=True)\n",
        "\n",
        "# Verify missing values after imputation\n",
        "remaining_missing = df[\"ConvertedCompYearly\"].isnull().sum()\n",
        "print(f\"Missing values in ConvertedCompYearly after imputation: {remaining_missing}\")\n",
        "\n",
        "# Summary statistics of normalized compensation\n",
        "print(\"\\nSummary Statistics of ConvertedCompYearly:\")\n",
        "print(df[\"ConvertedCompYearly\"].describe())\n",
        "\n",
        "# Save cleaned dataset (optional)\n",
        "df.to_csv(\"cleaned_survey_data.csv\", index=False)\n",
        "\n",
        "print(\"Compensation normalization complete. Cleaned dataset saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SwITe7gM0XUX",
        "outputId": "3ecd9db5-6a82-4bb0-ac50-6ae1ac53e3d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in ConvertedCompYearly: 42002\n",
            "Missing values in ConvertedCompYearly after imputation: 0\n",
            "\n",
            "Summary Statistics of ConvertedCompYearly:\n",
            "count    6.543700e+04\n",
            "mean     7.257636e+04\n",
            "std      1.122207e+05\n",
            "min      1.000000e+00\n",
            "25%      6.500000e+04\n",
            "50%      6.500000e+04\n",
            "75%      6.500000e+04\n",
            "max      1.625660e+07\n",
            "Name: ConvertedCompYearly, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b1da4108d1d2>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"ConvertedCompYearly\"].fillna(median_comp, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compensation normalization complete. Cleaned dataset saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What This Code Does**\n",
        "Checks for missing values in ConvertedCompYearly.\n",
        "Handles missing values by filling them with the median compensation (to avoid skewing from extreme values).\n",
        "Verifies that missing values are handled correctly.\n",
        "Displays summary statistics for further analysis."
      ],
      "metadata": {
        "id": "YLzVqEII1tNB"
      }
    }
  ]
}